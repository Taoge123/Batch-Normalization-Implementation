{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True, reshape=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hand-designed fully connected layers\n",
    "\n",
    "def fully_connected(prev_layer, num_units):\n",
    "    \n",
    "    layer = tf.layers.dense(prev_layer, num_units, activation=tf.nn.relu)\n",
    "    \n",
    "    return layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hand-designed convolutional layer\n",
    "\n",
    "def conv_layer(prev_layer, layer_depth):\n",
    "    \n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_depth * 4, 3, strides, 'same', activation=tf.nn.relu)\n",
    "    \n",
    "    return conv_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69105, Validation accuracy: 0.08680\n",
      "Batch: 25: Training loss: 0.39280, Training accuracy: 0.09375\n",
      "Batch: 50: Training loss: 0.33051, Training accuracy: 0.07812\n",
      "Batch: 75: Training loss: 0.32553, Training accuracy: 0.12500\n",
      "Batch: 100: Validation loss: 0.32607, Validation accuracy: 0.11260\n",
      "Batch: 125: Training loss: 0.32484, Training accuracy: 0.12500\n",
      "Batch: 150: Training loss: 0.32356, Training accuracy: 0.14062\n",
      "Batch: 175: Training loss: 0.32799, Training accuracy: 0.04688\n",
      "Batch: 200: Validation loss: 0.32524, Validation accuracy: 0.11260\n",
      "Batch: 225: Training loss: 0.32517, Training accuracy: 0.06250\n",
      "Batch: 250: Training loss: 0.32309, Training accuracy: 0.21875\n",
      "Batch: 275: Training loss: 0.32661, Training accuracy: 0.06250\n",
      "Batch: 300: Validation loss: 0.32524, Validation accuracy: 0.10700\n",
      "Batch: 325: Training loss: 0.32279, Training accuracy: 0.17188\n",
      "Batch: 350: Training loss: 0.32537, Training accuracy: 0.10938\n",
      "Batch: 375: Training loss: 0.32180, Training accuracy: 0.14062\n",
      "Batch: 400: Validation loss: 0.32609, Validation accuracy: 0.09900\n",
      "Batch: 425: Training loss: 0.32874, Training accuracy: 0.03125\n",
      "Batch: 450: Training loss: 0.32400, Training accuracy: 0.12500\n",
      "Batch: 475: Training loss: 0.32467, Training accuracy: 0.17188\n",
      "Batch: 500: Validation loss: 0.32524, Validation accuracy: 0.10020\n",
      "Batch: 525: Training loss: 0.32427, Training accuracy: 0.03125\n",
      "Batch: 550: Training loss: 0.32611, Training accuracy: 0.07812\n",
      "Batch: 575: Training loss: 0.32710, Training accuracy: 0.09375\n",
      "Batch: 600: Validation loss: 0.32567, Validation accuracy: 0.09580\n",
      "Batch: 625: Training loss: 0.32584, Training accuracy: 0.12500\n",
      "Batch: 650: Training loss: 0.32343, Training accuracy: 0.09375\n",
      "Batch: 675: Training loss: 0.32382, Training accuracy: 0.17188\n",
      "Batch: 700: Validation loss: 0.32552, Validation accuracy: 0.09900\n",
      "Batch: 725: Training loss: 0.32813, Training accuracy: 0.09375\n",
      "Batch: 750: Training loss: 0.32470, Training accuracy: 0.12500\n",
      "Batch: 775: Training loss: 0.32425, Training accuracy: 0.12500\n",
      "Final validation accuracy: 0.09580\n",
      "Final test accuracy: 0.09800\n",
      "Accuracy on 100 samples: 0.08\n"
     ]
    }
   ],
   "source": [
    "def train(num_batches, batch_size, learning_rate):\n",
    "    \n",
    "    #build placeholder for the input samples and labels\n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    #feed the inputs into a series of 20 convolutional layers\n",
    "    layer = inputs\n",
    "    \n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i)\n",
    "        \n",
    "    #Flatten the output from the convolutional layers\n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "    \n",
    "    #Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100)\n",
    "    \n",
    "    #Create the output layer with 1 node for each\n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    #define\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, \n",
    "                                 labels: batch_ys})\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually, just to make sure batch normalization really worked\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]]})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add batch normalization\n",
    "\n",
    "def fully_connected(prev_layer, num_units, is_training):\n",
    "    \n",
    "    layer = tf.layers.dense(prev_layer, num_units, use_bias=False, activation=None)\n",
    "    layer = tf.layers.batch_normalization(layer, training=is_training)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    \n",
    "    return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, layer_depth, is_training):\n",
    "    \n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    \n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_depth * 4, 3, strides, 'same', use_bias=False, activation=None)\n",
    "    conv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    return conv_layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69082, Validation accuracy: 0.08680\n",
      "Batch: 25: Training loss: 0.55938, Training accuracy: 0.06250\n",
      "Batch: 50: Training loss: 0.44163, Training accuracy: 0.12500\n",
      "Batch: 75: Training loss: 0.37767, Training accuracy: 0.06250\n",
      "Batch: 100: Validation loss: 0.34406, Validation accuracy: 0.08680\n",
      "Batch: 125: Training loss: 0.33836, Training accuracy: 0.07812\n",
      "Batch: 150: Training loss: 0.33407, Training accuracy: 0.06250\n",
      "Batch: 175: Training loss: 0.35065, Training accuracy: 0.07812\n",
      "Batch: 200: Validation loss: 0.31078, Validation accuracy: 0.23640\n",
      "Batch: 225: Training loss: 0.21772, Training accuracy: 0.53125\n",
      "Batch: 250: Training loss: 0.13925, Training accuracy: 0.78125\n",
      "Batch: 275: Training loss: 0.05201, Training accuracy: 0.90625\n",
      "Batch: 300: Validation loss: 0.08067, Validation accuracy: 0.86720\n",
      "Batch: 325: Training loss: 0.06269, Training accuracy: 0.90625\n",
      "Batch: 350: Training loss: 0.05916, Training accuracy: 0.92188\n",
      "Batch: 375: Training loss: 0.06272, Training accuracy: 0.90625\n",
      "Batch: 400: Validation loss: 0.06215, Validation accuracy: 0.89820\n",
      "Batch: 425: Training loss: 0.04486, Training accuracy: 0.90625\n",
      "Batch: 450: Training loss: 0.01097, Training accuracy: 0.98438\n",
      "Batch: 475: Training loss: 0.01200, Training accuracy: 0.98438\n",
      "Batch: 500: Validation loss: 0.02885, Validation accuracy: 0.95900\n",
      "Batch: 525: Training loss: 0.02001, Training accuracy: 0.96875\n",
      "Batch: 550: Training loss: 0.05054, Training accuracy: 0.95312\n",
      "Batch: 575: Training loss: 0.05034, Training accuracy: 0.93750\n",
      "Batch: 600: Validation loss: 0.05400, Validation accuracy: 0.92000\n",
      "Batch: 625: Training loss: 0.06224, Training accuracy: 0.93750\n",
      "Batch: 650: Training loss: 0.00656, Training accuracy: 0.98438\n",
      "Batch: 675: Training loss: 0.03937, Training accuracy: 0.96875\n",
      "Batch: 700: Validation loss: 0.04413, Validation accuracy: 0.93860\n",
      "Batch: 725: Training loss: 0.04057, Training accuracy: 0.96875\n",
      "Batch: 750: Training loss: 0.05203, Training accuracy: 0.92188\n",
      "Batch: 775: Training loss: 0.02272, Training accuracy: 0.95312\n",
      "Final validation accuracy: 0.92780\n",
      "Final test accuracy: 0.92150\n",
      "Accuracy on 100 samples: 0.93\n"
     ]
    }
   ],
   "source": [
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels \n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    # Add placeholder to indicate whether or not we're training the model\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "    # Feed the inputs into a series of 20 convolutional layers \n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i, is_training)\n",
    "\n",
    "    # Flatten the output from the convolutional layers \n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "\n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100, is_training)\n",
    "\n",
    "    # Create the output layer with 1 node for each \n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    # Define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    # Tell TensorFlow to update the population statistics while training\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    # Create operations to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys, is_training: True})\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels,\n",
    "                                                              is_training: False})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys, is_training: False})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels, \n",
    "                                  is_training: False})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels,\n",
    "                                  is_training: False})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually, just to make sure batch normalization really worked\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]],\n",
    "                                                    is_training: False})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fuly_connected(prev_layer, num_units, is_training):\n",
    "    \n",
    "    layer = tf.layers.dense(prev_layer, num_units, use_bias=False, activation=None)\n",
    "    \n",
    "    gamma = tf.Variable(tf.ones([num_units]))\n",
    "    beta = tf.Variable(tf.zeros([num_units]))\n",
    "    \n",
    "    pop_mean = tf.Variable(tf.zeros([num_units]), trainable=False)\n",
    "    pop_variance = tf.Variable(tf.ones([num_units]), training=False)\n",
    "    \n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    def batch_norm_training():\n",
    "        batch_mean, batch_variance = tf.nn.moments(layer, [0])\n",
    "        \n",
    "        decay = 0.99\n",
    "        \n",
    "        train_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_variance = tf.Variable(tf.ones([num_units]), trainable=False)\n",
    "        \n",
    "        with tf.control_dependencies([train_mean, train_variance]):\n",
    "            return tf.nn.batch_bormalization(layer, batch_mean, batch_variance, beta, gamma, epsilon)\n",
    "       \n",
    "    \n",
    "    def batch_norm_inference():\n",
    "        return tf.nn.batch_normalization(layer, pop_mean, pop_variance, beta, gamma, epsilon)\n",
    "    \n",
    "    batch_normalized_output = tf.cond(is_training, batch_norm_training, batch_norm_inference)\n",
    "    return tf.nn.relu(batch_normalization_output)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, layer_depth, is_training):\n",
    "\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    \n",
    "    in_channels = prev_layer.get_shape().as_list()[3]\n",
    "    out_channels = layer_depth*4\n",
    "    \n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([3, 3, in_channels, out_channels], stddev=0.05))\n",
    "    \n",
    "    layer = tf.nn.conv2d(prev_layer, weights, strides=[1,strides, strides, 1], padding='SAME')\n",
    "\n",
    "    gamma = tf.Variable(tf.ones([out_channels]))\n",
    "    beta = tf.Variable(tf.zeros([out_channels]))\n",
    "\n",
    "    pop_mean = tf.Variable(tf.zeros([out_channels]), trainable=False)\n",
    "    pop_variance = tf.Variable(tf.ones([out_channels]), trainable=False)\n",
    "\n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    def batch_norm_training():\n",
    "        # Important to use the correct dimensions here to ensure the mean and variance are calculated \n",
    "        # per feature map instead of for the entire layer\n",
    "        batch_mean, batch_variance = tf.nn.moments(layer, [0,1,2], keep_dims=False)\n",
    "\n",
    "        decay = 0.99\n",
    "        train_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_variance = tf.assign(pop_variance, pop_variance * decay + batch_variance * (1 - decay))\n",
    "\n",
    "        with tf.control_dependencies([train_mean, train_variance]):\n",
    "            return tf.nn.batch_normalization(layer, batch_mean, batch_variance, beta, gamma, epsilon)\n",
    " \n",
    "    def batch_norm_inference():\n",
    "        return tf.nn.batch_normalization(layer, pop_mean, pop_variance, beta, gamma, epsilon)\n",
    "\n",
    "    batch_normalized_output = tf.cond(is_training, batch_norm_training, batch_norm_inference)\n",
    "    return tf.nn.relu(batch_normalized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69119, Validation accuracy: 0.09580\n",
      "Batch: 25: Training loss: 0.61182, Training accuracy: 0.04688\n",
      "Batch: 50: Training loss: 0.54801, Training accuracy: 0.09375\n",
      "Batch: 75: Training loss: 0.49691, Training accuracy: 0.14062\n",
      "Batch: 100: Validation loss: 0.45821, Validation accuracy: 0.09900\n",
      "Batch: 125: Training loss: 0.43098, Training accuracy: 0.07812\n",
      "Batch: 150: Training loss: 0.39582, Training accuracy: 0.07812\n",
      "Batch: 175: Training loss: 0.40146, Training accuracy: 0.09375\n",
      "Batch: 200: Validation loss: 0.37252, Validation accuracy: 0.09860\n",
      "Batch: 225: Training loss: 0.36705, Training accuracy: 0.06250\n",
      "Batch: 250: Training loss: 0.38049, Training accuracy: 0.07812\n",
      "Batch: 275: Training loss: 0.41582, Training accuracy: 0.10938\n",
      "Batch: 300: Validation loss: 0.43021, Validation accuracy: 0.13620\n",
      "Batch: 325: Training loss: 0.49166, Training accuracy: 0.14062\n",
      "Batch: 350: Training loss: 0.52607, Training accuracy: 0.23438\n",
      "Batch: 375: Training loss: 0.31102, Training accuracy: 0.46875\n",
      "Batch: 400: Validation loss: 0.22991, Validation accuracy: 0.58460\n",
      "Batch: 425: Training loss: 0.20182, Training accuracy: 0.59375\n",
      "Batch: 450: Training loss: 0.16251, Training accuracy: 0.73438\n",
      "Batch: 475: Training loss: 0.06104, Training accuracy: 0.87500\n",
      "Batch: 500: Validation loss: 0.06437, Validation accuracy: 0.88580\n",
      "Batch: 525: Training loss: 0.07180, Training accuracy: 0.87500\n",
      "Batch: 550: Training loss: 0.02971, Training accuracy: 0.93750\n",
      "Batch: 575: Training loss: 0.04746, Training accuracy: 0.93750\n",
      "Batch: 600: Validation loss: 0.03894, Validation accuracy: 0.94300\n",
      "Batch: 625: Training loss: 0.04189, Training accuracy: 0.93750\n",
      "Batch: 650: Training loss: 0.06940, Training accuracy: 0.84375\n",
      "Batch: 675: Training loss: 0.01961, Training accuracy: 0.96875\n",
      "Batch: 700: Validation loss: 0.04527, Validation accuracy: 0.92720\n",
      "Batch: 725: Training loss: 0.02128, Training accuracy: 0.96875\n",
      "Batch: 750: Training loss: 0.03594, Training accuracy: 0.95312\n",
      "Batch: 775: Training loss: 0.06441, Training accuracy: 0.92188\n",
      "Final validation accuracy: 0.90100\n",
      "Final test accuracy: 0.89810\n",
      "Accuracy on 100 samples: 0.9\n"
     ]
    }
   ],
   "source": [
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels \n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    # Add placeholder to indicate whether or not we're training the model\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "    # Feed the inputs into a series of 20 convolutional layers \n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i, is_training)\n",
    "\n",
    "    # Flatten the output from the convolutional layers \n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "\n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100, is_training)\n",
    "\n",
    "    # Create the output layer with 1 node for each \n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    # Define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    # Create operations to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys, is_training: True})\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels,\n",
    "                                                              is_training: False})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys, is_training: False})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels, \n",
    "                                  is_training: False})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels,\n",
    "                                  is_training: False})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually, just to make sure batch normalization really worked\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]],\n",
    "                                                    is_training: False})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
